=============================
A Brief Tour of Symbolic PyMC
=============================

    :Author: Brandon T. Willard
    :Date: 2019-08-03



1 Introduction
--------------

In this document we'll cover the basics of the Symbolic PymC package while
implementing a symbolic "search-and-replace" for terms
like ``tf.matmul(A, x + y)``
into ``tf.matmul(A, x) + tf.matmul(A, y)``.  In other words, we will
demonstrate how to implement the distributive property of matrix multiplication
so that it can be applied to arbitrary TensorFlow graphs.

Symbolic PyMC allows one to implement rewrite rules like the distributive
property, as well as other sophisticated symbolic manipulations of graphs,
through a combination of fundamental computational concepts with direct and
transparent implementations in pure Python.

Throughout the course of our implementation, we will briefly introduce these concepts
and explicitly show how they relate to graph manipulations and the high-level math
that they model.


We start by creating a graph of our target
expressions--i.e. ``tf.matmul(A, x + y``--in TensorFlow.  We do this so
that we can inspect its structure and determine exactly what we need to search
for and replace.

.. code:: python
    :name: tf-matmul-graph

    import numpy as np

    import tensorflow as tf

    from IPython.lib.pretty import pprint

    from tensorflow.python.eager.context import graph_mode


    with graph_mode():
        # Matrix
        A_tf = tf.compat.v1.placeholder(tf.float64, name='A',
                                        shape=tf.TensorShape([None, None]))
        # Column vectors
        x_tf = tf.compat.v1.placeholder(tf.float64, name='x',
                                        shape=tf.TensorShape([None, 1]))
        y_tf = tf.compat.v1.placeholder(tf.float64, name='y',
                                        shape=tf.TensorShape([None, 1]))
        # The multiplication
        z_tf = tf.matmul(A_tf, x_tf + y_tf)

A text print-out of the full TensorFlow graph is provided by the debug print
function ``tf_dprint``.

.. code:: python
    :name: tf-print-graph

    from symbolic_pymc.tensorflow.printing import tf_dprint

    tf_dprint(z_tf)

.. code:: python

    Tensor(MatMul):0,	shape=[None, 1]	"MatMul_5:0"
    |  Op(MatMul)	"MatMul_5"
    |  |  Tensor(Placeholder):0,	shape=[None, None]	"A_1:0"
    |  |  Tensor(Add):0,	shape=[None, 1]	"add_3:0"
    |  |  |  Op(Add)	"add_3"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"x_1:0"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"y_1:0"

The output of Listing `tf-print-graph`_ shows us the underlying operators (e.g. ``MatMul``,
``Placeholder``, ``Add``) and their arguments.

To "match/search for" combinations of TensorFlow operations--or, in other words, graphs--like
`tf-print-graph`_, we use `**unification** <https://en.wikipedia.org/wiki/Unification_(computer_science)>`_; to "replace" parts of a graph (well, to produce copies with
replaced parts), we use `**reification** <https://en.wikipedia.org/wiki/Reification_(computer_science)>`_.  Symbolic PyMC provides support for
these using `TensorFlow <https://www.tensorflow.org/>`_ (and `Theano <http://deeplearning.net/software/theano/>`_) graphs via **meta objects** and **expression-tuples**.

2 Meta Objects
--------------

Meta objects model the essential components of TensorFlow graphs, while allowing one
to use input that isn't normally valid.  More specifically, we can
construct meta graphs that contain **logic variables**.  Later, those logic
variables can be replaced with other objects that allow the meta graph to
be converted into a real TensorFlow graph.

Existing TensorFlow graphs can be converted to their meta graph equivalents with
the ``mt`` helper object.

.. code:: python
    :name: convert-to-meta

    from symbolic_pymc.tensorflow.meta import mt


    z_mt = mt(z_tf)

.. code:: python
    :name: convert-to-meta-print

    tf_dprint(z_mt)

.. code:: python

    Tensor(MatMul):0,	shape=[None, 1]	"MatMul_5:0"
    |  Op(MatMul)	"MatMul_5"
    |  |  Tensor(Placeholder):0,	shape=[None, None]	"A_1:0"
    |  |  Tensor(Add):0,	shape=[None, 1]	"add_3:0"
    |  |  |  Op(Add)	"add_3"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"x_1:0"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"y_1:0"

A meta graph can be converted to a TensorFlow graph using its ``reify`` method.

.. code:: python
    :name: meta-to-tf

    tf_dprint(z_mt.reify())

.. code:: python

    Tensor(MatMul):0,	shape=[None, 1]	"MatMul:0"
    |  Op(MatMul)	"MatMul"
    |  |  Tensor(Placeholder):0,	shape=[None, None]	"A:0"
    |  |  Tensor(Add):0,	shape=[None, 1]	"add:0"
    |  |  |  Op(Add)	"add"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"x:0"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"y:0"

The ``mt`` object also makes it easier to construct meta graphs by hand.

.. code:: python
    :name: create-meta-graph

    from unification import unify, reify, var


    with graph_mode():
        add_mt = mt.add(1, var('a'))

.. code:: python

    pprint(add_mt)

.. code:: python

    TFlowMetaTensor(
      dtype=~_5,
      op=TFlowMetaOp(
        op_def=TFlowMetaOpDef(Add),
        node_def=TFlowMetaNodeDef( op='Add', name='add', attr=OrderedDict()),
        inputs=(TFlowMetaConstant( ), ~a),
        name='add'),
      value_index=0,
      shape=~_6,
      name='add:0')

In `create-meta-graph`_, we created a graph of ``1`` plus
a ``unification`` logic variable with the name ``'a'``. This
wouldn't be possible with a standard TensorFlow graph.

Also, because one of the elements in the graph is a logic variable, it cannot be
converted into a TensorFlow graph. Instead, if we attempt to use the meta
graph's ``reify`` method, we are simply given the meta graph back.

.. code:: python
    :name: bad-reify-meta-graphh

    pprint(add_mt.reify())

.. code:: python

    TFlowMetaTensor(
      dtype=~_5,
      op=TFlowMetaOp(
        op_def=TFlowMetaOpDef(Add),
        node_def=TFlowMetaNodeDef( op='Add', name='add', attr=OrderedDict()),
        inputs=(TFlowMetaConstant( ), ~a),
        name='add'),
      value_index=0,
      shape=~_6,
      name='add:0')

3 S-expressions
---------------

As an alternative approach to full meta graph conversion, we can also convert
TensorFlow graphs into an `S-expression-like <https://en.wikipedia.org/wiki/S-expression>`_ form.

.. code:: python
    :name: etuplize-graph

    from symbolic_pymc.etuple import etuple, etuplize


    z_sexp = etuplize(z_tf)

.. code:: python
    :name: etuplize-graph-print

    pprint(z_sexp)

.. code:: python

    ExpressionTuple((
      TFlowMetaOpDef(MatMul),
      ExpressionTuple((
        TFlowMetaOpDef(Placeholder),
        'float64',
        ExpressionTuple((
          symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
          (None, None))),
        'A_1')),
      ExpressionTuple((
        TFlowMetaOpDef(Add),
        ExpressionTuple((
          TFlowMetaOpDef(Placeholder),
          'float64',
          ExpressionTuple((
            symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
            (None, 1))),
          'x_1')),
        ExpressionTuple((
          TFlowMetaOpDef(Placeholder),
          'float64',
          ExpressionTuple((
            symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
            (None, 1))),
          'y_1')),
        'add_3')),
      False,
      False,
      'MatMul_5'))

By working with ``etuple``s, we can use arbitrary Python functions in
conjunction with meta graphs and logic variables. Basically,
an ``etuple`` can be manipulated until all of its
constituent logic variables and meta objects are eventually able to be converted
into concrete base objects, then the ``etuple`` can be evaluated.

For example, in Listing `etuple-eval-example`_, we create an ``etuple``
that uses the TensorFlow function ``tf.add`` with a logic variable argument.

.. code:: python
    :name: etuple-eval-example

    x_lv, y_lv = var('x'), var('y')

    add_tf_pat = etuple(tf.add, x_lv, y_lv)

Normally, this wouldn't be possible, as demonstrated in Listing `etuple-bad-usage-example`_.

.. code:: python
    :name: etuple-bad-usage-example

    try:
        tf.add(x_lv, 1)
    except ValueError as e:
        print(str(e))

.. code:: python

    Attempt to convert a value (~x) with an unsupported type (<class 'unification.variable.Var'>) to a Tensor.

We would get the same error if we attempted to evaluate the ``etuple``,
which is done by accessing the ``eval_obj`` property.  However, after
performing a simple manipulation that replaces the logic variable with valid
input to ``tf.add``, we are able to evaluate the ``etuple``, as
demonstrated in Listings `etuple-reify-example`_ and
`etuple-reify-eval-print-example`_.

.. code:: python
    :name: etuple-reify-example

    add_pat_new = reify(add_tf_pat, {x_lv: 1, y_lv: 1})

.. code:: python
    :name: etuple-reify-print-example

    pprint(add_pat_new)

.. code:: python

    ExpressionTuple((
      <function tensorflow.python.ops.gen_math_ops.add(x, y, name=None)>,
      1,
      1))

.. code:: python
    :name: etuple-reify-eval-print-example

    pprint(add_pat_new.eval_obj)

.. code:: python

    <tf.Tensor: id=34, shape=(), dtype=int32, numpy=2>

Working with S-expressions is much like manipulating a subset of Python AST, so
that--in effect--one can automate the production of TensorFlow-using code.

4 Unification and Reification
-----------------------------

With the ability to use logic variables and TensorFlow graphs together, we can
"search" arbitrary graphs using **unification** and produce new graphs by replacing
logic variables with previously unified results--via **reification**.

We start by making "patterns" or templates for the subgraphs we would like to match.
Patterns, in this case, take the form of meta graphs or S-expr graphs with the
desired structure and logic variables in place of "unknown" or arbitrary terms
that we might like to reference elsewhere.

Listing `matmul-pattern`_ represents an S-expr that evaluates to a graph in
which two terms are matrix-multiplied.

.. code:: python
    :name: matmul-pattern

    A_lv, B_lv = var('A'), var('B')

    matmul_pat = etuple(mt.matmul, A_lv, B_lv, False, False, var('name'))

The first two terms are logic variables, so they can be matched/unified with anything.

The remaining two booleans and name parameter are the optional arguments
to ``mt.matmul``, and they correspond to
the ``transpose_a``, ``transpose_b``, and name parameters of the
original ``tf.matmul`` operator (i.e. the operator modeled by the meta
operator ``mt.matmul``).  By setting the first two optional parameters
to non-logic variable values, we are effectively restricting the possible
matches.

"Matching" a graph against our pattern is accomplished with **unification**.
Unification of two graphs implies unification of all sub-graphs and elements
between them.  When unification is successful, it returns a map of logic
variables and their unified values.  If there are no logic variables in the
graphs--it simply returns an empty map.  If unification fails, it
returns ``False``--at least in the implementation we use, but not
necessarily in general.

4.1 Unification
~~~~~~~~~~~~~~~

We can perform the unification using the function ``unify``.  The result
is a ``dict`` mapping logic variables to their unified values.

.. code:: python
    :name: matmul-pattern-unify

    s = unify(matmul_pat, z_sexp, {})

.. code:: python
    :name: matmul-pattern-unify-print

    pprint(s)

.. code:: python

    {~A: ExpressionTuple((
       TFlowMetaOpDef(Placeholder),
       'float64',
       ExpressionTuple((
         symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
         (None, None))),
       'A')),
     ~B: ExpressionTuple((
       TFlowMetaOpDef(Add),
       ExpressionTuple((
         TFlowMetaOpDef(Placeholder),
         'float64',
         ExpressionTuple((
           symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
           (None, 1))),
         'x')),
       ExpressionTuple((
         TFlowMetaOpDef(Placeholder),
         'float64',
         ExpressionTuple((
           symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
           (None, 1))),
         'y')),
       'add')),
     ~name: 'MatMul'}

4.2 Reification
~~~~~~~~~~~~~~~

Using ``reify``, we can "fill-in"--or replace--the logic variables of
our "pattern" with the matches obtained by ``unify`` that are held
within the variable ``s``, or we could specify our own substitutions
based on that information.

In Listing `matmul-pattern-reify`_, we simply change the ``'name'`` value in the
and create a new graph with that value.  The end result is a version of the original
graph, ``z_sexp``, with a new name.

.. code:: python
    :name: matmul-pattern-reify

    s[var('name')] = 'a new name'

    z_sexp_re = reify(matmul_pat, s)

.. code:: python
    :name: matmul-pattern-reify-print

    pprint(z_sexp_re)

.. code:: python

    ExpressionTuple((
      TFlowMetaOpDef(MatMul),
      ExpressionTuple((
        TFlowMetaOpDef(Placeholder),
        'float64',
        ExpressionTuple((
          symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
          (None, None))),
        'A')),
      ExpressionTuple((
        TFlowMetaOpDef(Add),
        ExpressionTuple((
          TFlowMetaOpDef(Placeholder),
          'float64',
          ExpressionTuple((
            symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
            (None, 1))),
          'x')),
        ExpressionTuple((
          TFlowMetaOpDef(Placeholder),
          'float64',
          ExpressionTuple((
            symbolic_pymc.tensorflow.meta.TFlowMetaTensorShape,
            (None, 1))),
          'y')),
        'add')),
      False,
      False,
      'a new name'))

4.3 Finishing our Implementation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can also reify an entirely different graph using the values extracted
from the graph ``z_sexp``.  In this case, we create an "output" pattern graph, to
complement our "input" pattern graph, ``matmul_pat``.

If we combine our matrix multiplication and addition ``etuple`` patterns, we
can extract all the arguments needed as input to a distributed multiplication pattern.

.. code:: python
    :name: dist-new-pattern

    output_pat = etuple(mt.add,
                        etuple(mt.matmul, A_lv, x_lv),
                        etuple(mt.matmul, A_lv, y_lv))

With the correct assignments/mappings of the constituent logic variables, ``A_lv``,
``x_lv`` and ``y_lv``, we can reify ``output_pat`` and obtain
the desired graph.  From our earlier unification results in Listing `matmul-pattern-unify`_,
we only need to apply the addition pattern (i.e. virtually the same
as ``add_tf_pat``, just with ``mt.add`` instead
of ``tf.add``) to the unified valued of the second multiplication
pattern argument, ``B_lv``.

.. code:: python
    :name: dist-add-unify

    add_pat = etuple(mt.add, x_lv, y_lv, var())

    s_add = unify(s[B_lv], add_pat, s)

In Listing `dist-add-unify`_, we added an extra element to ``add_pat`` for
the name parameter, since--upon inspection--we can see that the sub-graph for
addition, ``s[B_lv]``, actually has four elements: the meta
operator ``mt.add``, two sub-sub-graphs that construct the addends
denoted by ``x`` and ``y``, and the output/operator name used by TensorFlow.
Without this extra element, the unification would fail, and--although we could've
use the exact name in our test/example graph, ``'add'``--using an
"anonymous" logic variable like we did is more generally applicable, because it
match **any** name.

Also, notice that we gave our previous unification results/logic variable mappings
to the ``unify`` function in Listing `dist-add-unify`_; we did this in order
to merge all the mappings for the logic variables referenced in our output pattern,
``output_pat``.

.. code:: python
    :name: dist-new-pattern-reify

    z_new = reify(output_pat, s_add)

.. code:: python
    :name: dist-new-pattern-reify-print

    tf_dprint(z_new.eval_obj)

.. code:: python

    Tensor(Add):0,	shape=[None, 1]	"Add_1:0"
    |  Op(Add)	"Add_1"
    |  |  Tensor(MatMul):0,	shape=[None, 1]	"MatMul_1:0"
    |  |  |  Op(MatMul)	"MatMul_1"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, None]	"A:0"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"x:0"
    |  |  Tensor(MatMul):0,	shape=[None, 1]	"MatMul_2:0"
    |  |  |  Op(MatMul)	"MatMul_2"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, None]	"A:0"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"y:0"


As we've seen, using only the basics of unification and reification provided by
Symbolic PyMC, one can extract specific elements from TensorFlow graphs and use
them to implement mathematical identities/relations.  Through clever use of
multiple mathematical relations, one can--for example--construct graph
**optimizations** that turn large classes of user-defined statistical models into
computational tractable reformulations.  Similarly, one can construct "normal forms"
for models, making it possible to determine whether or not a user-defined model
is suitable for a specific sampler.


Next, we'll introduce another major element of Symbolic PyMC that orchestrates
and simplifies sequences of unifications like we used earlier, provides
control-flow-like capabilities, produces fully reified results of arbitrary
form, and does so within a genuinely declarative formalism that carries much of
the same power as logic programming: `miniKanren <http://minikanren.org/>`_!

5 Relational Programming in miniKanren
--------------------------------------

As mentioned at the end of the last section, Symbolic PyMC uses a Python
implementation of the embedded domain-specific language miniKanren--provided by
the ``kanren`` package--to orchestrate more sophisticated uses of
unification and reification.  For a quick intro, see `the basic introduction <https://github.com/logpy/logpy/blob/master/doc/basic.md>`_
provided by the ``kanren`` package.  We'll cover most of the same
basic material here, but not all.

To start, miniKanren uses **goals** (in the same sense as `logic programming <https://en.wikipedia.org/wiki/Logic_programming>`_) to
assert relations, and the ``run`` function evaluates those goals and
allows one to specify the exact amount and type of reified output desired from
the **states** that satisfy the goals.

In their most basic form, miniKanren **states** are simply the substitution maps returned by
unification, which--in the normal course of operation--aren't dealt with directly.

5.1 The Basic Goals
~~~~~~~~~~~~~~~~~~~

Normally, a user will only need to construct compound goals from a basic set of
primitives.  Arguably, the most primitive goal is the `equivalence relation <https://en.wikipedia.org/wiki/Equivalence_relation>`_
under unification denoted by ``eq`` in Python.

In Listing , we ask for all successful results/reifications (signified
by the ``0`` argument) of the logic variable ``var('q')`` for the goal
``eq(var('q'), 1)``--i.e. unify ``var('q')`` with ``1``.

.. code:: python
    :name: mk-basics-eq

    from kanren import run, eq

    q_lv = var('q')
    mk_res = run(0, q_lv, eq(q_lv, 1))

.. code:: python
    :name: mk-basics-eq-print

    pprint(mk_res)

.. code:: python

    (1,)

Since miniKanren's ``run`` always returns a stream of results, we obtain
a tuple containing the reified value of ``q_lv`` under the one
possible state for which our stated goal successfully evaluates.

The other basic primitives represent conjunction and disjunction of miniKanren
goals: ``lall`` and ``lany``, respectively.

.. code:: python
    :name: mk-basics-lall

    from kanren import lall, lany

    mk_res = run(0, q_lv, lall(eq(q_lv, 1), eq(q_lv, 2)))

.. code:: python
    :name: mk-basics-lall-print

    pprint(mk_res)

.. code:: python

    ()

In Listing `mk-basics-lall`_, we used ``lall`` to obtain the conjunction of two unification goals.
Since we requested that the same logic variable be unified
with both ``1`` and ``2`` simultaneously (i.e. in the same
state), which isn't possible, we got back an empty stream of results--indicating failure.

Goal disjunction, ``lany``, will split a state stream across goals,
producing new distinct states for each.

.. code:: python
    :name: mk-basics-lany

    mk_res = run(0, q_lv, lany(eq(q_lv, 1), eq(q_lv, 2)))

.. code:: python
    :name: mk-basics-lany-print

    pprint(mk_res)

.. code:: python

    (1, 2)

The goal disjunction results in Listing `mk-basics-lany-print`_ show that the logic variable
``q_lv`` can be unified with either ``1`` **or** ``2`` under the
two unification goals.

A common pattern of disjunction and conjunction is called ``conde``, and
it mirrors the Lisp function ``cond``, which is effectively a type of
compound ``if ... elif ... elif ...``.  Specifically,
``conde([x_1, ...], ..., [y_1, ...])`` is the same as
``lany(lall(x_1, ...), ..., lall(y_1, ...))``--i.e. a disjunction of goal conjunctions.

.. code:: python
    :name: mk-basics-conde

    from kanren import conde

    r_lv = var('r')

    mk_res = run(0, [q_lv, r_lv],
                 conde(
                     [eq(q_lv, 1), eq(r_lv, 10)],
                     [eq(q_lv, 2), eq(r_lv, 20)],
                 ))

.. code:: python
    :name: mk-basics-conde-print

    pprint(mk_res)

.. code:: python

    ([1, 10], [2, 20])

In Listing `mk-basics-conde`_, we introduced another logic
variable, ``r_lv``, and requested the reified values of a list
containing both logic variables.  The output resembles the idea that
if ``q_lv`` is "equal" to ``1``, then ``r_lv`` is "equal"
to ``10``, etc.  Unlike normal conditionals, each clause/branch isn't
exclusive, instead each is realized when the goals in a branch can be successful.

Listing `mk-basics-conde-exclusive`_, demonstrates when ``conde`` can behave more
like a traditional conditional statement.

.. code:: python
    :name: mk-basics-conde-exclusive

    mk_res = run(0, [q_lv, r_lv],
                 lall(eq(q_lv, 1),
                      conde(
                          [eq(q_lv, 1), eq(r_lv, 10)],
                          [eq(q_lv, 2), eq(r_lv, 20)],
                      )))

.. code:: python
    :name: mk-basics-conde-exclusive-print

    pprint(mk_res)

.. code:: python

    ([1, 10],)

5.2 A Better Implementation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since miniKanren uses unification and reification, we can apply its basic goals
to TensorFlow graphs, as we did earlier, and reproduce the entire implementation
in a much more concise manner.

.. code:: python
    :name: mk-distribute

    mk_res = run(1, output_pat,
                 eq(matmul_pat, z_sexp),
                 eq(add_pat, B_lv))

.. code:: python
    :name: mk-distribute-print

    tf_dprint(mk_res[0].eval_obj)

.. code:: python

    Tensor(Add):0,	shape=[None, 1]	"Add_2:0"
    |  Op(Add)	"Add_2"
    |  |  Tensor(MatMul):0,	shape=[None, 1]	"MatMul_3:0"
    |  |  |  Op(MatMul)	"MatMul_3"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, None]	"A:0"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"x:0"
    |  |  Tensor(MatMul):0,	shape=[None, 1]	"MatMul_4:0"
    |  |  |  Op(MatMul)	"MatMul_4"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, None]	"A:0"
    |  |  |  |  Tensor(Placeholder):0,	shape=[None, 1]	"y:0"

We didn't need to use the goal conjunction operator ``lall`` explicitly
in Listing `mk-distribute`_, because all remaining goal arguments
to ``run`` are automatically applied in conjunction.

When combinations of miniKanren goals comprise logical units, we can wrap their
construction in a functions which we call **goal constructors**.
